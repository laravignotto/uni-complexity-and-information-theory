\chapter{Introduction}

Programma:
\begin{itemize}
    \item Teoria dell'Informazione
    \item Teoria della Complessità
    \item Algoritmi su Grafi ecc \dots
\end{itemize}





%%%%%%%%%%%%%%%%%%
\chapter{Teoria dell'Informazione}

\section{Entropia}
% LEZIONE 1
Claude Shannon, 1948, \textit{A Mathematical Theory of Communication}.

Un messaggio è una sequenza di lettere (simboli) da un alfabeto. Qual è l'informazione in una frase (messaggio)? Come possiamo misurare la quantità di informazione? Dipende dal contesto.

\subparagraph{Esempio} Il messaggio è ``Piove!''. Qual è la quantità di informazione? Per il signor Muller, che vive a Vienna, dove piove spesso, la quantità di informazione è bassa. Per Fatima, che vive nel deserto, invece, è alta.\bigskip

Si ha quindi che
\begin{itemize}
    \item Bassa probabilità di un evento 
\end{itemize}

...

% LEZIONE 2
\section{Teorema di Bayes}
Sappiamo che nel caso di due \textbf{eventi indipendenti}, la probabilità congiunta è
$$
    p(a_i,b_j) = p(a_i)\cdot p(b_j)
$$
Nel caso, invece, di due \textbf{eventi dipendenti} si ha
$$
    p(a_i,b_j) = p(a_i|b_j)\cdot p(b_j) = p(b_j|a_i)\cdot p(a_i)
$$
e quindi
$$
p(a_i|b_j) = \dfrac{p(b_j|a_i)\cdot p(a_i)}{p(b_j)}
$$
con $p(b_j)$ fattore di normalizzazione.

\textcolor{Red}{TODO: vedere al capitolo 2 del libro il prior, poterior, likelihood, ecc.}



\section{Proprietà dell'Entropia}
Libro, pag.~33. Le proprietà della funzione di entropia sono:
\begin{itemize}
    \item $H(P)\geq 0$ con uguaglianza iff $p_i=1$ per un qualche $i$. In particolare, $H(P)=0$ iff $\exists i ~p_i=1$, ovvero quando c'è un evento certo l'entropia è nulla.
    \item L'entropia è massimizzata se la distribuzione $p$ è uniforme.
\end{itemize}
Analizziamo meglio e dimostriamo la seconda proprietà.

\begin{property}[Entropia massima] 
    $$
        \mathcal{H}(P)\leq \log_2|P|
    $$
    con $|P|$ numero di eventi ($|X|$), e
    $$
        \mathcal{H}\left(\frac{1}{k},\frac{1}{k},\dots,\frac{1}{k}\right) = \log_2 k
    $$
    dove $k$ è il numero di eventi, ovvero $|P|$.
\end{property}

\subparagraph{Dimostrazione} La dimostrazione è basata su proprietà di funzioni complesse, in particolare sulla diseguaglianza di Jensen, la quale è una disuguaglianza che lega il valore di una funzione convessa al valore della medesima funzione calcolata nel valor medio del suo argomento.

Sia $\bm{f(x)=-x\log_2x}$ (cfr.~definizione di entropia). Vogliamo controllare se è concava o convessa, calcoliamo quindi la sua derivata seconda:
$$
    f''(x) = -\frac{1}{x}
$$
Sappiamo che $0\leq x\leq 1$ perché è una probabilità, e quindi abbiamo che $-\nicefrac{1}{x}<0$: la funzione è con\-ca\-va. Prendiamo ora due punti $x_1$ e $x_2$, e un punto $x$ tra i due.
% $0\leq-\nicefrac{1}{x}\leq 1$ perché è una probabilità, e quindi abbiamo che $x<0$: la funzione è concava. Prendiamo ora due punti $x_1$ e $x_2$, e un punto $x$ tra i due.

\textcolor{Red}{TODO: disegno}

Abbiamo che la media pesata di $x_1$ e $x_2$ è
$$
    x = \lambda x_1 + (1-\lambda)x_2 \qquad \text{con} \quad 0\leq\lambda\leq 1
$$
con $\lambda$ il peso. In particolare, se $\lambda=1$ allora $x=x_1$, se $\lambda=0$ allora $x=x_2$, e se $\lambda=\nicefrac{1}{2}$ allora $x$ si troverà esattamente a metà tra $x_1$ e $x_2$. Se prendiamo la combinazione lineare di $f(x_1)$ e $f(x_2)$, otteniamo la seguente di\-su\-gua\-glian\-za:
$$
    \lambda f(x_1) + (1-\lambda)f(x_2) \leq f(\lambda x_1 + (1-\lambda)x_2)
$$
Tale disuguaglianza si può generalizzare alla combinazione lineare di un qualsiasi numero di punti. Se $f''(x)\leq 0$, $\forall x\in[x_1,x_n]$ (ovvero $f''(x)$ è concava) si ha la disuguaglianza di Jensen: 
$$
\textcolor{ForestGreen}{\sum_{i=1}^n\lambda_if(x_i)} \leq \textcolor{Cerulean}{f\left(\sum_{i=1}^n\lambda_ix_i\right)}
$$
con $\lambda_i\geq0$ e $\sum_{i=1}^n\lambda_i=1$. La disuguaglianza cambia verso ($\geq$) quando la funzione è convessa, cioè $f''(x)\geq 0$. Ricordiamo che vogliamo arrivare alla combinazione lineare dove i punti hanno la stessa probabilità. Quindi con
$$
    \lambda_i=\frac{1}{k} \qquad\qquad |P|=|X|=k \qquad\qquad P=\{p_1,\dots,p_k\}
$$
scriviamo la disuguaglianza di Jensen come
\begin{eqnarray*}
    \textcolor{ForestGreen}{\sum_{i=1}^k\underbrace{\frac{1}{k}}_{\lambda_i}\underbrace{(-p_i\log_2p_i)}_{f(x_i)}} & \leq & \underbrace{\textcolor{Cerulean}{-\left(\sum_{i=1}^k\frac{1}{k}p_i\right)\log_2\left(\sum_{i=1}^k\frac{1}{k}p_i\right)}}_{f(\sum\lambda_ix_i)}\\
    -\cancelto{\text{\footnotesize semplifichiamo perché $k>0$}}{\frac{1}{k}}\sum_{i=1}^kp_i\log_2p_i & \leq & -\cancel{\frac{1}{k}}\log_2\frac{1}{k}\\
    \mathcal{H}(P) & \leq & \log_2k=\mathcal{H}\left(\frac{1}{k},\dots,\frac{1}{k}\right)
\end{eqnarray*}
Ovvero l'entropia è massima per la distribuzione uniforme. \hfill$\Box$\bigskip 

\begin{property}[Entropia Congiunta]
    Siano $P,Q$ due distribuzioni, e $x_i,y_j$ coppia di eventi tali che $x_i\in P$ e $y_j\in Q$. L'entropia congiunta di $P,Q$ è:
    $$
        \mathcal{H}(P,Q) = -\sum_{i,j}p(x_i,y_j)\log_2(p(x_i,y_j))
    $$
    Se $x$ e $y$ sono indipendenti (quindi la probabilità congiunta è il prodotto delle due probabilità) l'entropia è additiva:
    $$
        \mathcal{H}(P,Q) = \mathcal{H}(P) + \mathcal{H}(Q)
        % ESERCIZIO ESAME
    $$
\end{property}
La somma è possibile perché usiamo i logaritmi, e una delle loro proprietà è $\log(a\cdot b)=\log(a)+\log(b)$.


\subsection{Scomponibilità dell'Entropia}
Libro, pag.~33. Sia $P$ una distribuzione (vettore) di probabilità, e $X$ delle variabili.
\begin{eqnarray*}
    P &=& \{ p_1,~p_2,~...,~p_n\}\\
    X &=& \{ \underbrace{x_1}_{p_1},\underbrace{x_2,...,x_n}_{1-p_1}\}
\end{eqnarray*}
In questo contesto, la probabilità, ad esempio, del secondo evento $x_2$ è, normalizzata, pari a $\nicefrac{p_2}{1-p_1}$, e quella dell'ultimo elemento $x_n$ è $\nicefrac{p_n}{1-p_1}$.

\subparagraph{Esempio} Abbiamo una moneta regolare. Al primo lancio esce $H$, e come risultati desiderati per il secondo e terzo lancio vogliamo $T$ e $T$. Abbiamo quindi:
\begin{eqnarray*}
    \underbrace{H}_{\substack{p_1=\frac{1}{2}\\\frac{1}{2}}}~\underbrace{T\quad T}_{\substack{1-p_1=\frac{1}{2}\\\frac{1}{4}\quad\frac{1}{4}}}
\end{eqnarray*}

La quantità di informazione ricevuta da $P$ è uguale a quella ricevuta dal processo in due passaggi.
\begin{eqnarray*}
    \mathcal{H}(P)  & = & \sum p_i\log_2\frac{1}{p_i}\\
                    & = & \underbrace{\mathcal{H}(p_1,1-p_1)+(1-p_1)\cdot\mathcal{H}\underbrace{\left(\frac{p_2}{1-p_1},\frac{p_3}{1-p_1},\dots,\frac{p_n}{1-p_1}\right)}_{\substack{\text{$p_i$ normalizzati la cui somma è 1}\\1-p_1=p_2+p_3+\dots+p_n}}}_{\text{si possono dividere in diversi punti, ottenendo, ad esempio, tante entropie}}
\end{eqnarray*}

\textcolor{Red}{TODO: vdere proprietà libro pag 33 sez 2.6?}


\section{Inferenza}
\textcolor{Red}{TODO: capitolo 3, esercizio 3.8 pag 57}

\section{Compressione}
\textcolor{Red}{TODO: capitolo 4, esercizio 4.1 pag 66}


\section{Il Teorema Della Codifica Sorgente}
Studiamo $\mathcal{H}(\{p,1-p\})$, con $0\leq p\leq 1$
\begin{eqnarray*}
    \mathcal{H}(\{p,1-p\})  & = &\\
                            & = & p\log\frac{1}{p} + (1-p)\log\frac{1}{1-p}\\
                            & = & -p\log(p) - (1-p)\log(1-p)\\
                            & = & \mathcal{H}(p)
\end{eqnarray*}

\textcolor{Red}{TODO: finire}


\textcolor{Red}{LEZ 3}

\textcolor{Red}{TODO: soluzione esercizio 4.1}

\textcolor{Red}{TODO: muddy children puzzle}



\section{Codici Simbolo}

\begin{definition}[Alfabeti di input/output]
    \begin{align*}
        \text{Alfabeto di input} \quad &\mathcal{A}=\{a_1,a_2,\dots,a?\}\\
        \text{Alfabeto di output} \quad &\mathcal{B}=\{b_1,b_2,\dots,b?\}\\
    \end{align*}
\end{definition}

\begin{definition}[Codice]
    Sia $\mathcal{A}^*$ un messaggio (sequenza di caratteri) sull'alfabeto $\mathcal{A}$. Il \textbf{codice} $c$ è
    $$
        c:\mathcal{A}^*\to\mathcal{B}^*
    $$
    ovvero un messaggio dall'alfabeto $\mathcal{A}$ all'alfabeto $\mathcal{B}$.
\end{definition}
Con $\mathcal{A}^*=\bigcup_{n\in\mathbb{N}}A^n$, ovvero l'insieme di tutte le possibili stringhe che si possono creare utilizzando l'alfabeto $\mathcal{A}$, compresa la stringa vuota.\medskip 

Si vuole comprimere il messaggio in modo da ottenere il messaggio più corto possibile. Per farlo, utilizziamo una codifica.
\begin{definition}[Codifica]
    Una codifica è una funzione 
    $$
        \varphi:\mathcal{A}\to\mathcal{B}^*
    $$ 
    Inoltre
    $$
        \varphi(\underbrace{x_1,x_2,\dots,x_m}_{\in\mathcal{A}^*}) = \varphi(x_1)\varphi(x_2)\dots\varphi(x_m)
    $$
\end{definition}

\paragraph{Esempio 1} Alfabeti: $\mathcal{A}=\{a,b,c\}$, $\mathcal{B}=\{0,1\}$; codifica: $\varphi(a)=0$, $\varphi(b)=10$, $\varphi(c)=01$. È una buona codifica? No, perché è ambigua. Ad esempio
$$
    \varphi(ab) = 010 = \varphi(ca)
$$
È iniettiva nella codifica ma non sul messaggio. Una codifica di questo tipo viene detta \textbf{not uniquely decodable} (non univocamente decodificabile).

\begin{definition}[Univocamente decodificabile]
    Un codice $\varphi:\mathcal{A}\to\mathcal{B}^*$ è univocamente decodificabile (uniquely decodable) se
    $$
        \forall m_1,m_2\in\mathcal{A}^* \qquad \varphi(m_1)\neq\varphi(m_2)
    $$
\end{definition}
In questo corso non utilizzeremo codici non univocamente decodificabili.

\paragraph{Esempio 2} Alfabeti: $\mathcal{A}=\{a,b,c\}$, $\mathcal{B}=\{0,1\}$; codifica: $\varphi(a)=0$, $\varphi(b)=01$, $\varphi(c)=011$. Ad esempio, il messaggio $aabcbbca$ viene codificato come $\varphi(aabcbbca)=000101101010110$. È univocamente decodificabile (UD)?

Sì, è UD con delay 1. Ad ogni 0, si controlla il carattere successivo: se è un altro 0, la lettera è $a$, altrimenti si prosegue fino al primo 1 per decidere se è $b$ o $c$. Il delay 1 è riferito allo zero che si incontra, che significa l'inizio di un'altra lettera.

\paragraph{Esempio 3} Alfabeti: $\mathcal{A}=\{a,b,c\}$, $\mathcal{B}=\{0,1\}$; codifica: $\varphi(a)=00$, $\varphi(b)=1$, $\varphi(c)=10$. Ad esempio, il messaggio $bcbaca$ viene codificato come $\varphi(bcbaca)=1101001000$. È UD? 

Sì, si controlla se c'è un numero pari o dispari di 0 dopo un 1: se è pari, si tratta di una $b$ seguita da una o pià $a$, se è dispari di una $c$, eventualmente seguita da una o più $a$. È UD con unbounded delay.\bigskip 

Abbiamo visto che nel caso di distribuzione uniforme, la quantità di informazione è pari all'entropia. Quanto è complesso computare una codifica/decodifica?

Poiché l'alfabeto di input è binario, per rappresentare una codifica si può utilizzare un albero binario. Quello per l'\textbf{Esempio 2} è il seguente:
\begin{center}
    \begin{tikzpicture}[grow=right]
        \node {$\bullet$}
        child {
            node {$\bullet$}        
            edge from parent 
            node[below] {$1$}
        }
        child {
            node {$a$}        
            child {
                    node {$b$}
                    child {
                        node {$c$}
                        edge from parent
                        node[below] {$1$}
                    }
                    child {
                        node {$\bullet$}
                        edge from parent
                        node[above] {$0$}
                    }
                    edge from parent
                    node[below] {$1$}
                }
                child {
                    node {$\bullet$}
                    edge from parent
                    node[above] {$0$}
                }
            edge from parent         
            node[above] {$0$}
        };
    \end{tikzpicture}
\end{center}
Quello per l'\textbf{Esempio 3} è il seguente: \textcolor{Red}{TODO: fixare in modo che gli archi ad $a$ e $c$ siano inclinati}
\begin{center}
    \begin{tikzpicture}[grow=right]
        \node {$\bullet$}
        child {
            node {$b$}
            child {
                node {$c$}        
                edge from parent 
                node[below] {$0$}
            }        
            edge from parent 
            node[below] {$1$}
        }
        child {
            node {$\bullet$} 
            child {
                node {$a$}
                edge from parent
                node[above] {$0$}
            }
            edge from parent         
            node[above] {$0$}
        };
    \end{tikzpicture}
\end{center}
Quando si finisce in un nodo con un'etichetta ci si deve chiedere se la conclusione è che ci si può fermare. Il grado (fattore) di diramazione è pari alla cardinalità dell'alfabeto di output. Inoltre, se l'albero ha altezza $h$, tutte le codifiche hanno lunghezza $h$.

Ci chiediamo, qual è una codifica sicuramente UD e senza delay?

\begin{definition}[Codice prefisso]
    \begin{align*}
        \varphi : \mathcal{A} \to \mathcal{B}^* \text{ è un prefisso}\\
        \Updownarrow\\
        \forall a_i,a_j\in\mathcal{A} \quad \varphi(a_i)\text{ non è un prefisso di }\varphi(a_j)
    \end{align*}
\end{definition}
